import argparse
import os
import re
import json

import asyncio
import dotenv
from openai import AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential
from pydantic import BaseModel
from json_repair import repair_json


class Judge(BaseModel):
    """Comparison judgement model."""

    winner: int
    """The winner(1 or 2) of the comparison. 0 if the answers are equally good."""
    explanation: str
    """The explanation for the choice."""


COMPARE_PROMPT = """
---Role---

You are a helpful assistant that is trying to improve the quality of the answers generated by the system. 


---Goal---

You are given a question and a pair of answers generated by two different methods.
You are to choose which answer is better based on the given metric.

Analyse the answers and choose the winner based on the following metrics and provide a brief explanation for your choice.:
If the answers are equally good, set the winner to 0 and provide a brief explanation for your choice.

The response should be JSON formatted as follows:
{{
    "winner": 1,
    "explanation": "The answer from method 1 is more accurate and detailed. And ..."
}}

--Metric--

{metric}

---Question---

{question}

---Answer 1---

{answer1}

---Answer 2---

{answer2}


---Role---

You are a helpful assistant that is trying to improve the quality of the answers generated by the system. 


---Goal---

You are given a question and a pair of answers generated by two different methods.
You are to choose which answer is better based on the given metric.

Analyse the answers and choose the winner based on the following metrics and provide a brief explanation for your choice.:
If the answers are equally good, set the winner to 0 and provide a brief explanation for your choice.

The response should be JSON formatted as follows:
{{
    "winner": 1,
    "explanation": "The answer from method 1 is more accurate and detailed. And ..."
}}

"""

METHODS = ["s", "m"]

METRICS = [
    "Comprehensiveness.\nHow much detail does the answer provide to cover all aspects and details of the question?",
    "Diversity.\nHow varied and rich is the answer in providing different perspectives and insights on the question?",
    "Empowerment.\nHow well does the answer help the reader understand and make informed judgements about the topic?",
    "Directness.\nHow specifically and clearly does the answer address the question?",
]


def try_parse_json_object(input: str) -> tuple[str, dict]:
    """JSON cleaning and formatting utilities."""
    # Sometimes, the LLM returns a json string with some extra description, this function will clean it up.

    result = None
    try:
        # Try parse first
        result = json.loads(input)
    except json.JSONDecodeError:
        print("Warning: Error decoding faulty json, attempting repair")

    if result:
        return input, result

    pattern = r"\{(.*)\}"
    match = re.search(pattern, input, re.DOTALL)
    input = "{" + match.group(1) + "}" if match else input

    # Clean up json string.
    input = (
        input.replace("{{", "{")
        .replace("}}", "}")
        .replace('"[{', "[{")
        .replace('}]"', "}]")
        .replace("\\", " ")
        .replace("\\n", " ")
        .replace("\n", " ")
        .replace("\r", "")
        .strip()
    )

    # Remove JSON Markdown Frame
    if input.startswith("```json"):
        input = input[len("```json") :]
    if input.endswith("```"):
        input = input[: len(input) - len("```")]

    try:
        result = json.loads(input)
    except json.JSONDecodeError:
        # Fixup potentially malformed json string using json_repair.
        input = str(repair_json(json_str=input, return_objects=False))

        # Generate JSON-string output using best-attempt prompting & parsing techniques.
        try:
            result = json.loads(input)
        except json.JSONDecodeError:
            print("error loading json, json=%s", input)
            return input, {}
        else:
            if not isinstance(result, dict):
                print("not expected dict type. type=%s:", type(result))
                return input, {}
            return input, result
    else:
        return input, result


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def compare_answers(
    question: str, answer1: str, answer2: str, metric: str
) -> dict:
    try:
        response = await client.chat.completions.create(
            model="qwen-plus",
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "Judge",
                    "description": "Comparison judgement model.",
                    "schema": Judge.model_json_schema(),
                    "strict": True,
                },
            },
            messages=[
                {
                    "role": "user",
                    "content": COMPARE_PROMPT.format(
                        metric=metric,
                        question=question,
                        answer1=answer1,
                        answer2=answer2,
                    ),
                },
            ],
        )
        if isinstance(response.choices[0].message.content, str):
            try:
                res = json.loads(response.choices[0].message.content)
            except json.JSONDecodeError:
                _, res = try_parse_json_object(response.choices[0].message.content)
            except Exception as e:
                print(f"Error in compare_answers: {e}")
                return {"winner": 0, "explanation": "[LLM Error] No winner"}
            return res
    except Exception as e:
        print(f"Error in compare_answers: {e}")

    return {"winner": 0, "explanation": "[LLM Error] No winner"}


async def compare_batch(qas: list[dict]) -> list[dict]:
    winner_map = {1: "s", 2: "m", 0: "0"}
    # 构造所有问题所有metric的任务
    all_tasks = []
    for q in qas:
        for metric in METRICS:
            all_tasks.append(
                (
                    q,
                    metric,
                    asyncio.create_task(
                        compare_answers(
                            q["question"], q["s_answer"], q["m_answer"], metric
                        )
                    ),
                )
            )
    # 等待所有任务完成
    await asyncio.gather(*(t[2] for t in all_tasks))
    # 按问题聚合结果
    qid2metrics = {id(q): [] for q in qas}
    for q, metric, task in all_tasks:
        res = task.result()
        qid2metrics[id(q)].append(
            {
                "metric": metric.split(".")[0],
                "winner": winner_map.get(res.get("winner", 0), "0"),
                "explanation": res.get("explanation", ""),
            }
        )
    # 组装最终结构
    results = []
    for q in qas:
        results.append(
            {
                "question": q["question"],
                "s_answer": q.get("s_answer", ""),
                "m_answer": q.get("m_answer", ""),
                "metrics": qid2metrics[id(q)],
            }
        )
    return results


async def main():
    with open(args.input, "r", encoding="utf-8") as f:
        questions = json.load(f)
    batches = [
        questions[i : min(i + args.batch, len(questions))]
        for i in range(0, len(questions), args.batch)
    ]
    results = []
    for i, batch in enumerate(batches):
        try:
            batch_res = await compare_batch(batch)
        except Exception as e:
            print(f"Error in batch {i+1}: {e}")
            continue
        results.extend(batch_res)

        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(results, f)

        print(f"Batch {i+1}/{len(batches)} processed. {len(results)} results saved.")

    # statistics for two methods for each metric
    if args.compare:
        statistics = {m.split(".")[0]: {"s": 0, "m": 0, "0": 0} for m in METRICS}
        for q in results:
            for metric in q["metrics"]:
                statistics[metric["metric"]][metric["winner"]] += 1
        print("Statistics for each metric:")
        for metric, count in statistics.items():
            print(
                f"{metric}: {count["s"]} s wins, {count["m"]} m wins, {count["0"]} ties"
            )


def statistics_compare() -> dict:
    """Statistics for two methods for each metric."""
    with open(args.input, "r", encoding="utf-8") as f:
        qas = json.load(f)
    statistics = {m.split(".")[0]: {"s": 0, "m": 0, "0": 0} for m in METRICS}
    for q in qas:
        for metric in q["metrics"]:
            statistics[metric["metric"]][metric["winner"]] += 1
    print("Statistics for each metric:")
    for metric, count in statistics.items():
        print(f"{metric}: {count["s"]} s wins, {count["m"]} m wins, {count["0"]} ties")


if __name__ == "__main__":
    dotenv.load_dotenv()
    parser = argparse.ArgumentParser(
        description="Compare answers from different methods."
    )
    parser.add_argument(
        "-i",
        "--input",
        type=str,
        default="questions.json",
        help="The questions file to compare answers.",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default="compare.json",
        help="The output file to save the comparison results.",
    )
    parser.add_argument(
        "-b",
        "--batch",
        type=int,
        default=10,
        help="Batch size for processing questions.",
    )
    parser.add_argument(
        "-c", "--compare", action="store_true", help="Store statistics Compare answers"
    )
    parser.add_argument(
        "-s",
        "--statistics",
        action="store_true",
        help="Store statistics for each metric",
    )
    args = parser.parse_args()

    client = AsyncOpenAI(
        base_url=os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1"),
        api_key=os.environ.get("OPENAI_API_KEY", ""),
    )
    if args.statistics:
        statistics_compare()
    else:
        asyncio.run(main())


"""
python.exe -m tests.evaluation.compare -i tests/evaluation/search_res.json -o tests/evaluation/compare_res.json -b 10 -c
python.exe -m tests.evaluation.compare -i tests/evaluation/compare_res.json -s
"""
